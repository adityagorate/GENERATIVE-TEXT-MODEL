# GENERATIVE-TEXT-MODEL

*COMPANY*: CODTECH IT SOLUTION

*NAME*: ADITYA BHARAT GORATE

*INTERN ID*: CTIS3040

*DOMAIN*: AI

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTOSH

---

ğŸ“– Description

This project demonstrates a Generative AI Text Model that generates
coherent and meaningful paragraphs based on user-provided prompts or topics.

The model uses the pretrained **GPT-2 Transformer architecture**
from Hugging Face Transformers to generate human-like text without
training from scratch.

---

ğŸ¯ Objectives
- To understand generative text modeling
- To implement GPT-based text generation
- To generate paragraphs from user prompts
- To demonstrate AI text generation in a notebook environment

---

ğŸ› ï¸ Technologies Used
- Python
- Hugging Face Transformers
- PyTorch
- Jupyter Notebook / Google Colab

---

ğŸ“‚ Project Structure

Generative_Text_Model/

â”‚

â”œâ”€â”€ notebook/

â”‚ â””â”€â”€ text_generation_gpt.ipynb

â”‚
â”œâ”€â”€ src/

â”‚ â””â”€â”€ text_generator.py

â”‚

â”œâ”€â”€ requirements.txt

---

âš™ï¸ Installation & Setup

1. Clone or download the project

2. Install required libraries:

pip install -r requirements.txt

Or 

install manually:

pip install transformers torch

---

â–¶ï¸ How to Run

Option 1 â€” Notebook

Open text_generation_gpt.ipynb

Run all cells

Enter a topic when prompted

Option 2 â€” Python Script

python src/text_generator.py

---

ğŸ§ª Sample Output

<img width="1127" height="308" alt="Image" src="https://github.com/user-attachments/assets/d9b177cc-02ec-4d77-904b-54fcacab7eb7" />

---

âœ¨ Features
- Generates coherent paragraphs
- Accepts user prompts
- Uses pretrained GPT-2 model
- No training required
- Adjustable text length

---

âš ï¸ Limitations
- May generate incorrect facts
- Limited to pretrained knowledge
- Requires internet for first model download
